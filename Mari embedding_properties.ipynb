{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting properties of embeddings\n",
    "\n",
    "* We will look at some of the properties w2v embeddings have\n",
    "* Test them ourselves in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case you forgot, this is how one can load the embeddings\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load embedding model from a file\n",
    "# binary: True if saved as binary file (.bin), False if saved as text file (.vectors or .txt for example)\n",
    "# limit: How many words to read from the model\n",
    "model_english=KeyedVectors.load_word2vec_format(\"/home/bio/gigaword-and-wikipedia.bin\", binary=True, limit=100000) #koulun koneella home/bio\n",
    "model_finnish=KeyedVectors.load_word2vec_format(\"/home/bio/pb34_wf_200_v2_skgram.bin\", binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Most similar words for 'man':\")\n",
    "print(model_english.most_similar(\"man\",topn=10))\n",
    "print()\n",
    "print(\"Most similar words for 'woman':\")\n",
    "print(model_finnish.most_similar(\"woman\",topn=10))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping spaces\n",
    "\n",
    "* One of the more famous properties of the embeddings\n",
    "* Learn a **linear** mapping from one language to another\n",
    "* Can we replicate this?\n",
    "* Learn a network with a single dense output layer\n",
    "* English vector in -- Finnish vector out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "* Need English-Finnish pairs of words to train on\n",
    "* ...google translate, maybe?\n",
    "* Googling around finds this https://github.com/ssut/py-googletrans\n",
    "* ...unofficial API, will get your IP banned if overused, so let's be careful!\n",
    "* official API needs registration, etc.\n",
    "\n",
    "`pip3 install --user googletrans` #install the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator=Translator()\n",
    "translations=translator.translate([\"locomotive\",\"milk\"],src=\"en\",dest=\"fi\")\n",
    "\n",
    "for t in translations:\n",
    "    print(\"origin=\",t.origin,\"text=\",t.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Seems to work fine!\n",
    "* Let's grab some translations\n",
    "* The docs say \"max 16000 characters per request\"\n",
    "* We need to translate some hundreds of words at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab 100000\n",
      "<class 'dict'>\n",
      "Vocab(count:99365, index:635)\n",
      "[('</s>', <gensim.models.keyedvectors.Vocab object at 0x7f9e983e7da0>), (',', <gensim.models.keyedvectors.Vocab object at 0x7f9e983e7e80>), ('the', <gensim.models.keyedvectors.Vocab object at 0x7f9e98404198>), ('.', <gensim.models.keyedvectors.Vocab object at 0x7f9e984041d0>), ('of', <gensim.models.keyedvectors.Vocab object at 0x7f9e6fe60c18>)]\n",
      "Freq sorted ['</s>', ',', 'the', '.', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(\"English vocab\",len(model_english.vocab))\n",
    "print(model_english.vocab.__class__)\n",
    "print(model_english.vocab[\"car\"])\n",
    "#We need a list, in order of frequency\n",
    "words=sorted(model_english.vocab.items(),key=lambda word_dim:word_dim[1].count,reverse=True)\n",
    "print(words[:5])\n",
    "words_freq_sorted=[w for w,_ in words]\n",
    "print(\"Freq sorted\",words_freq_sorted[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have things like `</s>` and `.` in the vocabulary, those we don't want to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'to', 'and', 'in', 'a', 'for', 'The', 'is', 'that', 'was', 'on', 'with', 'said', 'as', 'by', 'at', 'from', 'he', 'his']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "english_word_re=re.compile(\"^[a-zA-Z]+$\") #about as stupid simplification as you can get! #^=after[x]\n",
    "final_word_list=[]\n",
    "for w in words_freq_sorted:\n",
    "    if english_word_re.match(w):\n",
    "        final_word_list.append(w)\n",
    "print(final_word_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Finnish ['ja', 'on', 'ei', 'että', 'se', 'oli', 'mutta', 'tai', 'kun', 'myös', 'ovat', 'ole', 'niin', 'jos', 'kuin'] ... ['seurakunnan', 'selvä', 'tulleet', 'seuraavaan', 'sijasta', 'kuollut', 'I', 'asioihin', 'loput', 'luona', 'talven', 'per', 'ihanan', 'palvelua', 'tietokoneen']\n",
      "Final English ['the', 'of', 'to', 'and', 'in', 'a', 'for', 'The', 'is', 'that', 'was', 'on', 'with', 'said', 'as'] ... ['Greece', 'rally', 'democracy', 'revenue', 'add', 'criticism', 'offices', 'Hussein', 'kids', 'relief', 'promised', 'advance', 'talking', 'boost', 'dispute']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#same thing as above, nicely packed into a function\n",
    "def clean_vocab(gensim_model,regexp):\n",
    "    words=sorted(gensim_model.vocab.items(),key=lambda word_dim:word_dim[1].count,reverse=True)\n",
    "    words_freq_sorted=[w for w,_ in words]\n",
    "    word_re=re.compile(regexp)\n",
    "    final_word_list=[]\n",
    "    for w in words_freq_sorted:\n",
    "        if word_re.match(w):\n",
    "            final_word_list.append(w)\n",
    "    return final_word_list\n",
    "\n",
    "finnish_vocab=clean_vocab(model_finnish,'^[a-zA-ZäöåÖÄÅ]+$')\n",
    "english_vocab=clean_vocab(model_english,'^[a-zA-Z]+$')\n",
    "print(\"Final Finnish\",finnish_vocab[:15],\"...\",finnish_vocab[2000:2015])\n",
    "print(\"Final English\",english_vocab[:15],\"...\",english_vocab[2000:2015])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Little test (run through the google translate)\n",
    "import time\n",
    "def translate(words,src,dest,batch_size=1000):\n",
    "    result=[] #[(\"dog\",\"koira\"),....]\n",
    "    translator=Translator()\n",
    "    for idx in range(0,len(words),batch_size):\n",
    "        batch=words[idx:idx+batch_size]\n",
    "        try:\n",
    "            translations=translator.translate(batch,src=src,dest=dest)\n",
    "            for t in translations:\n",
    "                result.append((t.origin,t.text))\n",
    "            time.sleep(0.2) #sleep between batches\n",
    "            print(src,\"->\",dest,\"batch at\",idx,\"....OK\")\n",
    "        except: #we end here, if the lines between try ... except throw an error\n",
    "            print(src,\"->\",dest,\"batch at\",idx,\"....FAILED\")\n",
    "            time.sleep(61) #sleep a little longer so Google is not angry\n",
    "            print(src,\"->\",dest,\"...RESTARTING\")\n",
    "            \n",
    "    return result\n",
    "\n",
    "x=translate(english_vocab[:50],\"en\",\"fi\",20) # a small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* looks okay\n",
    "* let's run this and save the result for later use, so we don't get banned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "en_fi=translate(english_vocab,\"en\",\"fi\",batch_size=150)\n",
    "with open(\"en_fi_transl.json\",\"wt\") as f:\n",
    "    json.dump(en_fi,f)\n",
    "fi_en=translate(finnish_vocab,\"fi\",\"en\",batch_size=150)\n",
    "with open(\"fi_en_transl.json\",\"wt\") as f:\n",
    "    json.dump(fi_en,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* well, we got banned :D\n",
    "* Let's just translate as text files, in the google translate interface\n",
    "* (quality time manually feeding these into Google translate --- I could have used the official API :)\n",
    "* ...but now it's done, so who cares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump 10K words at a time into a file, which can be fed to google translate\n",
    "def build_files(words,fname,batch_size):\n",
    "    for idx in range(0,len(words),batch_size):\n",
    "        batch=words[idx:idx+batch_size]\n",
    "        with open(\"trdata/{}_batch_{}.txt\".format(fname,idx),\"wt\") as f:\n",
    "            print(\"\\n\".join(batch),file=f)\n",
    "\n",
    "build_files(english_vocab,\"en-fi-source\",10000)\n",
    "build_files(finnish_vocab,\"fi-en-source\",10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I built manually four files like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trdata/enfi_source_all.txt\n",
      "trdata/enfi_target_all.txt\n",
      "trdata/fien_source_all.txt\n",
      "trdata/fien_target_all.txt\n",
      "  95275 trdata/fien_source_all.txt\n",
      "  95275 trdata/fien_target_all.txt\n",
      "  83618 trdata/enfi_source_all.txt\n",
      "  83618 trdata/enfi_target_all.txt\n",
      " 357786 total\n",
      "FI -> EN\n",
      "ja\tand\n",
      "on\tis\n",
      "ei\tNo\n",
      "että\tthat\n",
      "se\tit\n",
      "oli\twas\n",
      "mutta\tbut\n",
      "tai\tor\n",
      "kun\twhen\n",
      "myös\talso\n",
      "EN -> FI\n",
      "the\t\n",
      "of\tof\n",
      "to\tettä\n",
      "and\tja\n",
      "in\tsisään\n",
      "a\t\n",
      "for\tvarten\n",
      "The\t\n",
      "is\ton\n",
      "that\tettä\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls trdata/fien_* trdata/enfi_*\n",
    "wc -l trdata/fien_* trdata/enfi_*\n",
    "echo \"FI -> EN\"\n",
    "paste trdata/fien_source_all.txt trdata/fien_target_all.txt  | head -n 10 #(linuxissa) paste laittaa rivit vierekkäin\n",
    "echo \"EN -> FI\"\n",
    "paste trdata/enfi_source_all.txt trdata/enfi_target_all.txt  | head -n 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read in and filter\n",
    "* To make sure we get high-quality stuff, we will look for same pairs in fin-eng and eng-fin direction\n",
    "* That way we will also make sure our translations are among the top 100K words in both languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len fien 95275\n",
      "Len enfi 83610\n",
      "Len common 7100\n",
      "[('Selma', 'Selma'), ('kiiltävä', 'shiny'), ('yhdistynyt', 'united'), ('laajennettu', 'extended'), ('Jens', 'Jens'), ('Miguel', 'Miguel'), ('odottamaton', 'unexpected'), ('toimitukset', 'deliveries'), ('hyppysellinen', 'pinch'), ('puuha', 'chore'), ('syystä', 'justly'), ('psykologit', 'psychologists'), ('turkoosi', 'turquoise'), ('oikeudenmukainen', 'fair'), ('tekijä', 'factor'), ('sopivasti', 'suitably'), ('säilytyspaikka', 'repository'), ('Burton', 'Burton'), ('Sherman', 'Sherman'), ('Barry', 'Barry'), ('Carlson', 'Carlson'), ('ab', 'ab'), ('PG', 'PG'), ('ein', 'ein'), ('hukkaan', 'wasted'), ('säännöllisyys', 'regularity'), ('G', 'G'), ('koulu', 'school'), ('elinten', 'bodies'), ('raidallinen', 'striped'), ('taistelee', 'fights'), ('teollisesti', 'industrially'), ('leijonat', 'lions'), ('etana', 'snail'), ('talousarvio', 'budget'), ('des', 'des'), ('hetkiä', 'moments'), ('neuvonantaja', 'adviser'), ('sekaannus', 'confusion'), ('suitset', 'bridle'), ('outo', 'strange'), ('lounas', 'lunch'), ('uiminen', 'bathing'), ('Edvard', 'Edvard'), ('paviljonki', 'pavilion'), ('unelmat', 'dreams'), ('rasvat', 'fats'), ('kenkä', 'shoe'), ('Brian', 'Brian'), ('Eco', 'Eco'), ('mekaanisesti', 'mechanically'), ('Oliver', 'Oliver'), ('Forbes', 'Forbes'), ('vu', 'vu'), ('dos', 'dos'), ('sellainen', 'such'), ('ajaton', 'timeless'), ('aikana', 'during'), ('lihat', 'meats'), ('maalit', 'paints'), ('suu', 'mouth'), ('ärsyttävä', 'irritating'), ('kampaaja', 'hairdresser'), ('teoreettisesti', 'theoretically'), ('kerma', 'cream'), ('bulevardi', 'boulevard'), ('LT', 'LT'), ('viljellyt', 'cultivated'), ('teltat', 'tents'), ('D', 'D'), ('kipulääke', 'analgesic'), ('teologi', 'theologian'), ('syöpä', 'cancer'), ('rajoitteet', 'constraints'), ('ulkopuolella', 'outside'), ('peräkkäinen', 'consecutive'), ('Emanuel', 'Emanuel'), ('Mohamed', 'Mohamed'), ('Hillary', 'Hillary'), ('Seymour', 'Seymour'), ('Raymond', 'Raymond'), ('yleinen', 'general'), ('Seth', 'Seth'), ('BMI', 'BMI'), ('Pasi', 'Pasi'), ('Osbourne', 'Osbourne'), ('Rihanna', 'Rihanna'), ('Monroe', 'Monroe'), ('Charlie', 'Charlie'), ('un', 'un'), ('Agnes', 'Agnes'), ('Pius', 'Pius'), ('Frederik', 'Frederik'), ('TM', 'TM'), ('edistyminen', 'progress'), ('teoriat', 'theories'), ('Andorra', 'Andorra'), ('Pandora', 'Pandora'), ('virtaus', 'flow'), ('piikkejä', 'thorns'), ('verenvuoto', 'haemorrhage'), ('kertoa', 'tell'), ('Kylie', 'Kylie'), ('Lilly', 'Lilly'), ('Spears', 'Spears'), ('siirrot', 'transfers'), ('reunat', 'trimmings'), ('kuulustelut', 'interrogations'), ('Cristina', 'Cristina'), ('saukko', 'otter'), ('voitot', 'profits'), ('eurooppalaiset', 'Europeans'), ('keventää', 'lighten'), ('realistinen', 'realistic'), ('islam', 'Islam'), ('Kazakstan', 'Kazakstan'), ('takaisin', 'back'), ('soitti', 'phoned'), ('parannus', 'improvement'), ('suoristaa', 'straighten'), ('kevyesti', 'lightly'), ('pahin', 'worst'), ('kela', 'coil'), ('varmuuskopiot', 'backups'), ('hyppii', 'hopping'), ('kohtalainen', 'moderate'), ('keskikokoinen', 'medium'), ('leipuri', 'baker'), ('tukehtua', 'choke'), ('ylevä', 'lofty'), ('käänteinen', 'reverse'), ('Tennessee', 'Tennessee'), ('RPG', 'RPG'), ('Korea', 'Korea'), ('Edgar', 'Edgar'), ('Fleming', 'Fleming'), ('kudokset', 'tissues'), ('muna', 'egg'), ('Marriott', 'Marriott'), ('ninja', 'ninja'), ('tanssija', 'dancer'), ('välttää', 'avoid'), ('sosiaalisesti', 'socially'), ('huokaisi', 'sighed'), ('gamma', 'gamma'), ('iOS', 'iOS'), ('Caro', 'Caro'), ('Birmingham', 'Birmingham'), ('paranemista', 'healing'), ('EL', 'EL'), ('Köln', 'Cologne'), ('vastuussa', 'accountable'), ('totesi', 'stated'), ('voltti', 'volt'), ('Steelers', 'Steelers'), ('Finnair', 'Finnair'), ('OECD', 'OECD'), ('siepata', 'intercept'), ('HC', 'HC'), ('saumattomasti', 'seamlessly'), ('passit', 'passports'), ('peitot', 'blankets'), ('kasvohoito', 'facial'), ('nauhat', 'tapes'), ('synkkä', 'gloomy'), ('klooni', 'clone'), ('lehmä', 'cow'), ('makeanveden', 'freshwater'), ('perhe', 'family'), ('analysoida', 'analyze'), ('puutarhat', 'gardens'), ('ässää', 'aces'), ('asteikko', 'scale'), ('Leon', 'Leon'), ('betoni', 'concrete'), ('selvitys', 'statement'), ('italialainen', 'Italian'), ('urhea', 'brave'), ('Berlusconi', 'Berlusconi'), ('tulevaisuus', 'future'), ('tuuliajolla', 'adrift'), ('ryömii', 'crawls'), ('vastuullisuutta', 'accountability'), ('uusiutuva', 'renewable'), ('vetäytyi', 'withdrew'), ('firmware', 'firmware'), ('Ewa', 'Ewa'), ('kierrosta', 'rounds'), ('kahdeksankymmentä', 'eighty'), ('juhla', 'celebration'), ('MO', 'MO'), ('Roos', 'Roos'), ('vankila', 'prison'), ('Santo', 'Santo'), ('itkeä', 'cry'), ('epäsuora', 'indirect'), ('imeytyminen', 'absorption'), ('alus', 'vessel'), ('nauttiminen', 'enjoyment'), ('Mack', 'Mack'), ('tarkastus', 'inspection'), ('Antonio', 'Antonio'), ('määritelmät', 'definitions'), ('sekuntia', 'seconds'), ('Apache', 'Apache'), ('myyty', 'sold'), ('halvempaa', 'cheaper'), ('laimentaa', 'dilute'), ('kiharat', 'curls'), ('tarkoittaa', 'mean'), ('arvioitu', 'estimated'), ('samankaltainen', 'similar'), ('järkyttävän', 'shockingly'), ('uudistamalla', 'reforming'), ('Janeiro', 'Janeiro'), ('tennis', 'tennis'), ('tutkii', 'examines'), ('päivittäminen', 'updating'), ('häikäisevä', 'dazzling'), ('pihvit', 'steaks'), ('kokea', 'experience'), ('kiihtyvyys', 'acceleration'), ('kuntosali', 'gym'), ('uudistaa', 'renew'), ('luterilaisuus', 'Lutheranism'), ('kohdanneet', 'encountered'), ('oikea', 'right'), ('rangaistava', 'punishable'), ('kulttuurinen', 'cultural'), ('seitsemänkymmentä', 'seventy'), ('aiemmin', 'previously'), ('Lanka', 'Lanka'), ('AR', 'AR'), ('vaikutusvaltainen', 'influential'), ('oppituntia', 'lessons'), ('orjuus', 'slavery'), ('värillinen', 'colored'), ('huulet', 'lips'), ('Petter', 'Petter'), ('Armin', 'Armin'), ('Boston', 'Boston'), ('kastelu', 'irrigation'), ('Aston', 'Aston'), ('säätö', 'adjustment'), ('mytologia', 'mythology'), ('työmatkalainen', 'commuter'), ('jersey', 'jersey'), ('Gus', 'Gus'), ('kello', 'clock'), ('neulat', 'needles'), ('suolainen', 'salty'), ('Sean', 'Sean'), ('yksinkertainen', 'simple'), ('vertauskuvallisesti', 'metaphorically'), ('päivää', 'days'), ('johto', 'management'), ('värjäys', 'dyeing'), ('jompikumpi', 'either'), ('kevät', 'spring'), ('Dorothy', 'Dorothy'), ('Joan', 'Joan'), ('päättää', 'decide'), ('Luiz', 'Luiz'), ('kuljettaa', 'carry'), ('hän', 'he'), ('Sub', 'Sub'), ('Oscar', 'Oscar'), ('UP', 'UP'), ('ylös', 'up'), ('Symantec', 'Symantec'), ('pommi', 'bomb'), ('juniorit', 'juniors'), ('Dustin', 'Dustin'), ('hitaasti', 'slowly'), ('kisko', 'rail'), ('korvaus', 'compensation'), ('valokuvaajat', 'photographers'), ('kasvi', 'plant'), ('Newcastle', 'Newcastle'), ('voisi', 'could'), ('Felipe', 'Felipe'), ('Tomas', 'Tomas'), ('enimmäkseen', 'mostly'), ('rokote', 'vaccine'), ('kaikkitietävä', 'omniscient'), ('ripustaa', 'hang'), ('mikroaaltouuni', 'microwave'), ('värit', 'colors'), ('Mon', 'Mon'), ('orvoksi', 'orphaned'), ('nolla', 'zero'), ('Reebok', 'Reebok'), ('ammattimaisesti', 'professionally'), ('rukous', 'prayer'), ('Brandt', 'Brandt'), ('laukaus', 'shot'), ('suihkulähde', 'fountain'), ('Ludvig', 'Louis'), ('yleisurheilu', 'athletics'), ('sulhanen', 'groom')]\n"
     ]
    }
   ],
   "source": [
    "fien=[] #list of (fin,eng) pairs obtained from the fin -> eng direction\n",
    "enfi=[] #list of (fin,eng) pairs, this time obtained from  the eng->fin direction\n",
    "with open(\"trdata/fien_source_all.txt\") as fi_file, open(\"trdata/fien_target_all.txt\") as en_file:\n",
    "    for fi,en in zip(fi_file,en_file):\n",
    "        fi=fi.strip()\n",
    "        en=en.strip()\n",
    "        if fi and en:\n",
    "            fien.append((fi,en))\n",
    "\n",
    "with open(\"trdata/enfi_target_all.txt\") as fi_file, open(\"trdata/enfi_source_all.txt\") as en_file:\n",
    "    for fi,en in zip(fi_file,en_file):\n",
    "        fi=fi.strip()\n",
    "        en=en.strip()\n",
    "        if fi and en:\n",
    "            enfi.append((fi,en))\n",
    "\n",
    "fien_set=set(fien)\n",
    "enfi_set=set(enfi)\n",
    "common=fien_set&enfi_set #keep only pairs which are shared (&)\n",
    "print(\"Len fien\",len(fien_set))\n",
    "print(\"Len enfi\",len(enfi_set))\n",
    "print(\"Len common\",len(common))\n",
    "print(list(common)[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ouch - we lost most of the stuff, but such is life\n",
    "* what we got looks good, though :)\n",
    "* Let us yet filter away pairs like Ivan - Ivan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure all we found is in the top 100K - just crosschecking really\n",
    "print(len(set(finnish_vocab)&set(fi for fi,en in common)))\n",
    "print(len(set(english_vocab)&set(en for fi,en in common)))\n",
    "\n",
    "#Making sure all words are there exactly once - no risk of mixing train and validation\n",
    "print(len(set(fi for fi,en in common)))\n",
    "print(len(set(en for fi,en in common)))\n",
    "print(\"...all these four numbers should be the same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left with 4624 after removing identical pairs\n",
      "Shuffled pairs [('rivit', 'ranks'), ('mökki', 'cottage'), ('kuljettajat', 'drivers'), ('heittää', 'throw'), ('liikuttunut', 'touched'), ('salaisuudet', 'secrets'), ('kurja', 'wretched'), ('arvovaltainen', 'authoritative'), ('kirkkaus', 'brightness'), ('yksi', 'one'), ('muodostettu', 'formed'), ('kynttilä', 'candle'), ('nostot', 'withdrawals'), ('yhdiste', 'compound'), ('käyttämätön', 'unused'), ('yhdistys', 'association'), ('hätä', 'emergency'), ('ihailijoita', 'admirers'), ('innoissaan', 'excited'), ('jäätyy', 'freezes')]\n",
      "Indices: [4000, 15918, 3366, 3812, 6611, 8645, 40806, 22247, 32016, 53] [13168, 11185, 21928, 1751, 86387, 23331, 20705, 63390, 22397, 91]\n",
      "English model.vectors shape: (100000, 200)\n",
      "Finnish model.vectors shape: (100000, 200)\n",
      "English selected vectors shape: (4624, 200)\n",
      "Finnish selected vectors shape: (4624, 200)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "pairs=[(fi,en) for fi,en in common if fi!=en] #Only keep pairs where source does not equal target\n",
    "print(\"Left with\",len(pairs),\"after removing identical pairs\")\n",
    "random.shuffle(pairs) #always, always make sure to shuffle!\n",
    "\n",
    "print(\"Shuffled pairs\",pairs[:20])\n",
    "\n",
    "#Now we need to grab the vectors for the words in question\n",
    "en_indices=[model_english.vocab[en].index for fi,en in pairs] #English\n",
    "fi_indices=[model_finnish.vocab[fi].index for fi,en in pairs] #Finnish\n",
    "print(\"Indices:\",en_indices[:10],fi_indices[:10])\n",
    "#...and the vectors are hidden in the models\n",
    "print(\"English model.vectors shape:\",model_english.vectors.shape)\n",
    "print(\"Finnish model.vectors shape:\",model_finnish.vectors.shape)\n",
    "en_vectors=model_english.vectors[en_indices] #Selects the rows in just the correct order\n",
    "fi_vectors=model_finnish.vectors[fi_indices] #Selects the rows in just the correct order\n",
    "print(\"English selected vectors shape:\",en_vectors.shape)\n",
    "print(\"Finnish selected vectors shape:\",fi_vectors.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now `en_vectors` is vectors for the 4624 English words in our translation pairs\n",
    "* `fi_vectors` is same for Finnish\n",
    "* ...our training data is done - we have the pairs of input--desired output\n",
    "\n",
    "## Learning transformation from English to Finnish (fi->en)\n",
    "\n",
    "\n",
    "* 200-dim vector in, 200-dim vector out\n",
    "* Loss needs to be different, this is not classification!\n",
    "* `mse` stands for mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marsalv/.local/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               40200     \n",
      "=================================================================\n",
      "Total params: 40,200\n",
      "Trainable params: 40,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4161 samples, validate on 463 samples\n",
      "Epoch 1/30\n",
      "4161/4161 [==============================] - 0s 74us/step - loss: 0.0758 - val_loss: 0.0571\n",
      "Epoch 2/30\n",
      "4161/4161 [==============================] - 0s 61us/step - loss: 0.0494 - val_loss: 0.0426\n",
      "Epoch 3/30\n",
      "4161/4161 [==============================] - 0s 53us/step - loss: 0.0382 - val_loss: 0.0353\n",
      "Epoch 4/30\n",
      "4161/4161 [==============================] - 0s 47us/step - loss: 0.0321 - val_loss: 0.0311\n",
      "Epoch 5/30\n",
      "4161/4161 [==============================] - 0s 46us/step - loss: 0.0285 - val_loss: 0.0285\n",
      "Epoch 6/30\n",
      "4161/4161 [==============================] - 0s 54us/step - loss: 0.0263 - val_loss: 0.0268\n",
      "Epoch 7/30\n",
      "4161/4161 [==============================] - 0s 75us/step - loss: 0.0249 - val_loss: 0.0257\n",
      "Epoch 8/30\n",
      "4161/4161 [==============================] - 0s 78us/step - loss: 0.0239 - val_loss: 0.0250\n",
      "Epoch 9/30\n",
      "4161/4161 [==============================] - 0s 73us/step - loss: 0.0233 - val_loss: 0.0245\n",
      "Epoch 10/30\n",
      "4161/4161 [==============================] - 0s 66us/step - loss: 0.0228 - val_loss: 0.0242\n",
      "Epoch 11/30\n",
      "4161/4161 [==============================] - 0s 75us/step - loss: 0.0225 - val_loss: 0.0240\n",
      "Epoch 12/30\n",
      "4161/4161 [==============================] - 0s 67us/step - loss: 0.0223 - val_loss: 0.0238\n",
      "Epoch 13/30\n",
      "4161/4161 [==============================] - 0s 65us/step - loss: 0.0222 - val_loss: 0.0237\n",
      "Epoch 14/30\n",
      "4161/4161 [==============================] - 0s 59us/step - loss: 0.0221 - val_loss: 0.0236\n",
      "Epoch 15/30\n",
      "4161/4161 [==============================] - 0s 87us/step - loss: 0.0220 - val_loss: 0.0236\n",
      "Epoch 16/30\n",
      "4161/4161 [==============================] - 0s 73us/step - loss: 0.0219 - val_loss: 0.0236\n",
      "Epoch 17/30\n",
      "4161/4161 [==============================] - 0s 51us/step - loss: 0.0219 - val_loss: 0.0235\n",
      "Epoch 18/30\n",
      "4161/4161 [==============================] - 0s 60us/step - loss: 0.0219 - val_loss: 0.0235\n",
      "Epoch 19/30\n",
      "4161/4161 [==============================] - 0s 75us/step - loss: 0.0218 - val_loss: 0.0235\n",
      "Epoch 20/30\n",
      "4161/4161 [==============================] - 0s 86us/step - loss: 0.0218 - val_loss: 0.0235\n",
      "Epoch 21/30\n",
      "4161/4161 [==============================] - 0s 78us/step - loss: 0.0218 - val_loss: 0.0235\n",
      "Epoch 22/30\n",
      "4161/4161 [==============================] - 0s 51us/step - loss: 0.0218 - val_loss: 0.0235\n",
      "Epoch 23/30\n",
      "4161/4161 [==============================] - 0s 52us/step - loss: 0.0218 - val_loss: 0.0235\n",
      "Epoch 24/30\n",
      "4161/4161 [==============================] - 0s 57us/step - loss: 0.0218 - val_loss: 0.0235\n",
      "Epoch 25/30\n",
      "4161/4161 [==============================] - 0s 51us/step - loss: 0.0218 - val_loss: 0.0235\n",
      "Epoch 26/30\n",
      "4161/4161 [==============================] - 0s 66us/step - loss: 0.0218 - val_loss: 0.0235\n",
      "Epoch 27/30\n",
      "4161/4161 [==============================] - 0s 80us/step - loss: 0.0218 - val_loss: 0.0235\n",
      "Epoch 28/30\n",
      "4161/4161 [==============================] - 0s 83us/step - loss: 0.0218 - val_loss: 0.0235\n",
      "Epoch 29/30\n",
      "4161/4161 [==============================] - 0s 83us/step - loss: 0.0218 - val_loss: 0.0235\n",
      "Epoch 30/30\n",
      "4161/4161 [==============================] - 0s 67us/step - loss: 0.0218 - val_loss: 0.0235\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "### ---end of weird stuff\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "\n",
    "\n",
    "inp=Input(shape=(fi_vectors.shape[1],)) #input is 200-dim (eng to fin) (en->fi ja vice versa)\n",
    "outp=Dense(en_vectors.shape[1])(inp) #Simple linear transformation of the input (en->fi ja vice versa)\n",
    "\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss=\"mse\")\n",
    "hist=model.fit(fi_vectors,en_vectors,batch_size=100,verbose=1,epochs=30,validation_split=0.1) (en->fi ja vice versa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('reititin', 1.0000001192092896)]\n",
      "[('router', 1.0)]\n",
      "[('router', 0.8483683466911316), ('modem', 0.8167172074317932), ('adapter', 0.8042836785316467), ('Bluetooth', 0.7913797497749329), ('Ethernet', 0.7832474708557129)]\n",
      "\n",
      "\n",
      "[('indeksi', 1.0)]\n",
      "[('index', 1.0000001192092896)]\n",
      "[('ratio', 0.6710296273231506), ('weighting', 0.6368964910507202), ('value', 0.6021862030029297), ('P/E', 0.5989536643028259), ('calculated', 0.5913202166557312)]\n",
      "\n",
      "\n",
      "[('ihastunut', 0.9999997615814209)]\n",
      "[('enamored', 0.9999999403953552)]\n",
      "[('infatuated', 0.7519022822380066), ('smitten', 0.7447301149368286), ('envious', 0.731027364730835), ('obsessed', 0.7230865955352783), ('fascinated', 0.7076319456100464)]\n",
      "\n",
      "\n",
      "[('liikkua', 1.0000001192092896)]\n",
      "[('move', 1.0000001192092896)]\n",
      "[('keep', 0.6287741661071777), ('crawl', 0.6268678307533264), ('walk', 0.6221586465835571), ('roam', 0.6202652454376221), ('wander', 0.6198495030403137)]\n",
      "\n",
      "\n",
      "[('nälkä', 0.9999998211860657)]\n",
      "[('hunger', 1.0)]\n",
      "[('eating', 0.7361111640930176), ('eat', 0.6766142249107361), ('craving', 0.6685258746147156), ('cravings', 0.6674361824989319), ('smell', 0.6533191800117493)]\n",
      "\n",
      "\n",
      "[('vaatimus', 0.9999999403953552)]\n",
      "[('requirement', 0.9999998807907104)]\n",
      "[('requirement', 0.7043168544769287), ('interpretation', 0.6519451141357422), ('objection', 0.6510796546936035), ('limitation', 0.644322395324707), ('provision', 0.6355453729629517)]\n",
      "\n",
      "\n",
      "[('uskoa', 0.9999998807907104)]\n",
      "[('believe', 1.0)]\n",
      "[('believe', 0.7300664782524109), ('understand', 0.701062798500061), ('realize', 0.6879808902740479), ('appreciate', 0.6695892810821533), ('presume', 0.6657074093818665)]\n",
      "\n",
      "\n",
      "[('kiinnitetty', 1.0)]\n",
      "[('mortgaged', 1.0000001192092896)]\n",
      "[('fastened', 0.7215079069137573), ('affixed', 0.6641005277633667), ('mounted', 0.6420818567276001), ('attached', 0.6401515603065491), ('molded', 0.6246508359909058)]\n",
      "\n",
      "\n",
      "[('tiukasti', 1.0)]\n",
      "[('strictly', 1.0000001192092896)]\n",
      "[('rigidly', 0.6155163645744324), ('firmly', 0.6120497584342957), ('bent', 0.5874829888343811), ('tightly', 0.572725772857666), ('clearly', 0.5717548131942749)]\n",
      "\n",
      "\n",
      "[('kierrosta', 0.9999999403953552)]\n",
      "[('rounds', 1.0)]\n",
      "[('laps', 0.5713455677032471), ('rounds', 0.5505430698394775), ('races', 0.5496635437011719), ('lengths', 0.5453684329986572), ('seconds', 0.5348857641220093)]\n",
      "\n",
      "\n",
      "[('ehdotus', 1.0)]\n",
      "[('proposal', 1.0)]\n",
      "[('proposal', 0.7915483117103577), ('proposals', 0.7120835185050964), ('recommendation', 0.6904225945472717), ('proposed', 0.6439264416694641), ('recommendations', 0.6247458457946777)]\n",
      "\n",
      "\n",
      "[('pyörteisiin', 1.0)]\n",
      "[('swirling', 1.0)]\n",
      "[('effortlessly', 0.5751815438270569), ('romanticism', 0.5708062052726746), ('cliches', 0.569861114025116), ('fantasies', 0.5697548389434814), ('enchantment', 0.568565845489502)]\n",
      "\n",
      "\n",
      "[('pukeutunut', 1.0000001192092896)]\n",
      "[('dressed', 0.9999999403953552)]\n",
      "[('dressed', 0.8011578917503357), ('attired', 0.7764875292778015), ('wearing', 0.7677615880966187), ('demure', 0.7436795830726624), ('clad', 0.7340437769889832)]\n",
      "\n",
      "\n",
      "[('oivallus', 0.9999998807907104)]\n",
      "[('realization', 1.0000001192092896)]\n",
      "[('metaphor', 0.6933910250663757), ('paradox', 0.6924867033958435), ('intuition', 0.6892669200897217), ('sense', 0.6802628636360168), ('vision', 0.6776515245437622)]\n",
      "\n",
      "\n",
      "[('tuote', 1.0000001192092896)]\n",
      "[('product', 1.0)]\n",
      "[('packaging', 0.7199152112007141), ('products', 0.7013128995895386), ('high-quality', 0.6576023101806641), ('packaged', 0.6489835977554321), ('inexpensive', 0.6413500905036926)]\n",
      "\n",
      "\n",
      "[('housut', 0.9999999403953552)]\n",
      "[('pants', 0.9999998807907104)]\n",
      "[('tights', 0.8855372667312622), ('sandals', 0.8796020150184631), ('pants', 0.8753867149353027), ('socks', 0.8752942681312561), ('trousers', 0.8661218881607056)]\n",
      "\n",
      "\n",
      "[('luettelo', 1.0)]\n",
      "[('catalog', 0.9999998807907104)]\n",
      "[('lists', 0.6452332735061646), ('list', 0.5819650888442993), ('document', 0.567642331123352), ('item', 0.5666490793228149), ('collection', 0.5636062622070312)]\n",
      "\n",
      "\n",
      "[('ideologia', 1.0000001192092896)]\n",
      "[('ideology', 0.9999999403953552)]\n",
      "[('ideology', 0.8369191288948059), ('liberalism', 0.7980895638465881), ('egalitarianism', 0.7904640436172485), ('nationalism', 0.77153480052948), ('ideologies', 0.7711475491523743)]\n",
      "\n",
      "\n",
      "[('mutteri', 0.9999999403953552)]\n",
      "[('nut', 1.000000238418579)]\n",
      "[('spring-loaded', 0.8143156170845032), ('blade', 0.8043739795684814), ('pulley', 0.779247522354126), ('plunger', 0.7627831101417542), ('knob', 0.7598508596420288)]\n",
      "\n",
      "\n",
      "[('suututtaa', 1.0)]\n",
      "[('antagonize', 1.0)]\n",
      "[('annoy', 0.7970973253250122), ('despise', 0.7096343636512756), ('belittle', 0.7053846120834351), ('embarrass', 0.700247585773468), ('scold', 0.6891030669212341)]\n",
      "\n",
      "\n",
      "[('autotalli', 0.9999998807907104)]\n",
      "[('garage', 1.000000238418579)]\n",
      "[('garage', 0.7961205244064331), ('patio', 0.7478775978088379), ('kitchen', 0.7470522522926331), ('garages', 0.7386813163757324), ('fireplace', 0.7253690958023071)]\n",
      "\n",
      "\n",
      "[('johdonmukaisuus', 1.0)]\n",
      "[('consistency', 1.0000001192092896)]\n",
      "[('consistency', 0.760737955570221), ('objectivity', 0.7312900424003601), ('coherence', 0.7308577299118042), ('competence', 0.7211453914642334), ('uniformity', 0.7140266299247742)]\n",
      "\n",
      "\n",
      "[('vetämällä', 1.000000238418579)]\n",
      "[('pulling', 1.000000238418579)]\n",
      "[('blade', 0.7305989861488342), ('rope', 0.7169144153594971), ('spring-loaded', 0.7099708318710327), ('bending', 0.7001476287841797), ('fastened', 0.6974588632583618)]\n",
      "\n",
      "\n",
      "[('järjestäjät', 1.000000238418579)]\n",
      "[('organizers', 1.0000001192092896)]\n",
      "[('organizers', 0.7010776996612549), ('participants', 0.6787304878234863), ('attendees', 0.6445355415344238), ('athletes', 0.6345075368881226), ('organisers', 0.6320813298225403)]\n",
      "\n",
      "\n",
      "[('kiimainen', 1.0)]\n",
      "[('horny', 0.9999998211860657)]\n",
      "[('rambunctious', 0.748084545135498), ('giggle', 0.739402711391449), ('giggling', 0.7370478510856628), ('mischievous', 0.7315491437911987), ('wide-eyed', 0.7304894328117371)]\n",
      "\n",
      "\n",
      "[('heidän', 0.9999999403953552)]\n",
      "[('their', 1.000000238418579)]\n",
      "[('parents', 0.6057240962982178), ('willingly', 0.5884559154510498), ('wives', 0.5628014802932739), ('citizens', 0.5586104393005371), ('caregivers', 0.5472362041473389)]\n",
      "\n",
      "\n",
      "[('sinkki', 0.9999999403953552)]\n",
      "[('zinc', 1.0)]\n",
      "[('water-soluble', 0.8199625015258789), ('cellulose', 0.8119823336601257), ('selenium', 0.8027545809745789), ('calcium', 0.8006293773651123), ('antioxidants', 0.7931196689605713)]\n",
      "\n",
      "\n",
      "[('luonto', 1.0)]\n",
      "[('nature', 0.9999998807907104)]\n",
      "[('unspoiled', 0.7293717265129089), ('scenery', 0.7144886255264282), ('ecosystem', 0.7035037875175476), ('ecosystems', 0.6963229775428772), ('pristine', 0.693140983581543)]\n",
      "\n",
      "\n",
      "[('pankki', 0.9999998807907104)]\n",
      "[('bank', 1.0)]\n",
      "[('securities', 0.6765705347061157), ('banking', 0.6557557582855225), ('debt', 0.6298125982284546), ('bank', 0.6229152679443359), ('credit-card', 0.6217935085296631)]\n",
      "\n",
      "\n",
      "[('iho', 1.0)]\n",
      "[('skin', 1.0000001192092896)]\n",
      "[('skin', 0.8929424285888672), ('hair', 0.8091920614242554), ('discoloration', 0.7855064868927002), ('gums', 0.7824324369430542), ('wrinkled', 0.774367094039917)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_fi,val_en,_=hist.validation_data #This we saw before - the validation data\n",
    "predicted_en=model.predict(val_fi) #Transform the Finnish vectors in the validation data\n",
    "for fi,en,pred_en in list(zip(val_fi,val_en,predicted_en))[:30]:\n",
    "    print(model_finnish.similar_by_vector(fi,topn=1)) #This is the target English word\n",
    "    print(model_english.similar_by_vector(en,topn=1)) #This is the original Finnish word\n",
    "    print(model_english.similar_by_vector(pred_en,topn=5)) # Top five closest hits to the transformed vector\n",
    "    print(\"\\n\")\n",
    "    #en->fi ja vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating more formally\n",
    "\n",
    "* Eyeballing the data is a moving target\n",
    "* Ideally, we'd have a more solid metric\n",
    "* Let us try top-1, top-5, and top-10 for the proportion of words which got the correct translation among top-N candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 32.6133909287257 percent correct\n",
      "Top5 53.131749460043196 percent correct\n",
      "Top10 59.611231101511876 percent correct\n"
     ]
    }
   ],
   "source": [
    "def eval(src_model,tgt_model,src_vecs,tgt_vecs,predicted_vecs):\n",
    "    top1,top5,top10,total=0,0,0,0\n",
    "    for src_v,tgt_v,pred_v in zip(src_vecs,tgt_vecs,predicted_vecs):\n",
    "        src_word=src_model.similar_by_vector(src_v)[0][0]\n",
    "        tgt_word=tgt_model.similar_by_vector(tgt_v)[0][0]\n",
    "        hits=list(w for w,sim in tgt_model.similar_by_vector(pred_v,topn=10))\n",
    "        total+=1\n",
    "        if tgt_word==hits[0]:\n",
    "            top1+=1\n",
    "        if tgt_word in hits[:5]:\n",
    "            top5+=1\n",
    "        if tgt_word in hits[:10]:\n",
    "            top10+=1\n",
    "    print(\"Top1\",top1/total*100,\"percent correct\")\n",
    "    print(\"Top5\",top5/total*100,\"percent correct\")\n",
    "    print(\"Top10\",top10/total*100,\"percent correct\")\n",
    "eval(model_finnish,model_english,val_fi,val_en,predicted_en) #en->fi ja vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* We have seen the vectors have interesting properties\n",
    "* In particular, spaces can be mapped onto each other\n",
    "* We have seen how this can be achieved with a simple linear transformation\n",
    "* Optimal transformation has a closed-form solution, but we were lazy and trained it with Keras quite successfully\n",
    "* This demonstrates how Keras can be used also for more generic tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word> koira\n",
      "Cannot retrieve vector for koira\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ed66a3319589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"    WARNING: this word was seen during training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mhits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_english\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_finnish\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"  \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Extra stuff - a function to query the translations, so we can play around\n",
    "def top_n(word,source_model,target_model,transformation_model,topn=5):\n",
    "    try:\n",
    "        source_idx=source_model.vocab[word].index\n",
    "    except:\n",
    "        print(\"Cannot retrieve vector for\",word)\n",
    "        return None\n",
    "    mapped=transformation_model.predict(source_model.vectors[source_idx,:].reshape(1,-1))\n",
    "    return target_model.similar_by_vector(mapped[0])\n",
    "    \n",
    "seen_words=set(en for fi,en in common) #These words were seen during training or validation\n",
    "while True:\n",
    "    wrd=input(\"word> \")\n",
    "    if wrd==\"end\":\n",
    "        break\n",
    "    if wrd in seen_words:\n",
    "        print(\"    WARNING: this word was seen during training\")\n",
    "    hits=top_n(wrd,model_english,model_finnish,model)\n",
    "    for word,sim in hits:\n",
    "        print(\"  \",word,\"  \",sim)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
